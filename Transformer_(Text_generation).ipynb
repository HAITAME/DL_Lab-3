{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "name": "Transformer (Text generation)",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1457,
          "sourceType": "datasetVersion",
          "datasetId": 781
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAITAME/DL_Lab-3/blob/main/Transformer_(Text_generation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'short-jokes:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F781%2F1457%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240408%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240408T044309Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D085c30a7bd04463b2729feb4612b143bcb10b09476b54255fbe22364b7cd414307bf6d33c4a8f355594939bec552ca5d00d657f38c2cb45495e9f7f9c5acd1b05169f95a34e0a73d9809295476bb6654c1350bcbff73aac672ad1c8e92cebd049182dce230b16175cb230d9ee7ca5f861507e8e55cf987fc2a084e8dc63560e5d803ccad6e4a4f7dcd8ed7bfa38bf917193913650ae20cc339316fe8fe33013368ebf182f8645529bd2d22219cdd3f5400dd7d81b32119019860e897610bf310580b603433bb2488f4939734b44c3253062495f83b2dd7d54512048c3a51b19838e77ca2762392a9e6b7a256c26b194824622ed645bccf3ab21634a87e91b0ff'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ub8YVvH_BGzv"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "import torch\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "ntR1DiHtYW0I",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:56:45.284569Z",
          "iopub.execute_input": "2024-04-07T22:56:45.285212Z",
          "iopub.status.idle": "2024-04-07T22:56:53.561759Z",
          "shell.execute_reply.started": "2024-04-07T22:56:45.285178Z",
          "shell.execute_reply": "2024-04-07T22:56:53.560885Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "y1hiXsOMZffl",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:56:53.563283Z",
          "iopub.execute_input": "2024-04-07T22:56:53.563705Z",
          "iopub.status.idle": "2024-04-07T22:56:53.619856Z",
          "shell.execute_reply.started": "2024-04-07T22:56:53.563666Z",
          "shell.execute_reply": "2024-04-07T22:56:53.618884Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
      ],
      "metadata": {
        "id": "4OCNIadbYoIu",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:56:53.621342Z",
          "iopub.execute_input": "2024-04-07T22:56:53.62168Z",
          "iopub.status.idle": "2024-04-07T22:57:04.390184Z",
          "shell.execute_reply.started": "2024-04-07T22:56:53.621652Z",
          "shell.execute_reply": "2024-04-07T22:57:04.389454Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "g8gvFOL5bVi_",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:04.392346Z",
          "iopub.execute_input": "2024-04-07T22:57:04.392623Z",
          "iopub.status.idle": "2024-04-07T22:57:04.970991Z",
          "shell.execute_reply.started": "2024-04-07T22:57:04.392599Z",
          "shell.execute_reply": "2024-04-07T22:57:04.969691Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)"
      ],
      "metadata": {
        "id": "Fe7wifbKbiDF",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:04.97282Z",
          "iopub.execute_input": "2024-04-07T22:57:04.973544Z",
          "iopub.status.idle": "2024-04-07T22:57:04.981057Z",
          "shell.execute_reply.started": "2024-04-07T22:57:04.973507Z",
          "shell.execute_reply": "2024-04-07T22:57:04.980089Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, jokes_dataset_path = '/kaggle/input/short-jokes'):\n",
        "        super().__init__()\n",
        "\n",
        "        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n",
        "\n",
        "        self.joke_list = []\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "\n",
        "        with open(short_jokes_path) as csv_file:\n",
        "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "\n",
        "            x = 0\n",
        "            for row in csv_reader:\n",
        "                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n",
        "                self.joke_list.append(joke_str)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.joke_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.joke_list[item]"
      ],
      "metadata": {
        "id": "ZWlvAArBeFhi",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:04.98463Z",
          "iopub.execute_input": "2024-04-07T22:57:04.984975Z",
          "iopub.status.idle": "2024-04-07T22:57:05.687258Z",
          "shell.execute_reply.started": "2024-04-07T22:57:04.984932Z",
          "shell.execute_reply": "2024-04-07T22:57:05.686017Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = JokesDataset()\n",
        "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "DD8pXFj3cHFX",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:05.688559Z",
          "iopub.execute_input": "2024-04-07T22:57:05.689343Z",
          "iopub.status.idle": "2024-04-07T22:57:06.394397Z",
          "shell.execute_reply.started": "2024-04-07T22:57:05.689313Z",
          "shell.execute_reply": "2024-04-07T22:57:06.39323Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "# from transformers import AdamW, WarmupLinearSchedule\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = 'cpu'kn\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "metadata": {
        "id": "eOT1zLOQea8f",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:06.395742Z",
          "iopub.execute_input": "2024-04-07T22:57:06.396054Z",
          "iopub.status.idle": "2024-04-07T22:57:06.416595Z",
          "shell.execute_reply.started": "2024-04-07T22:57:06.396028Z",
          "shell.execute_reply": "2024-04-07T22:57:06.415503Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "\n",
        "    for idx,joke in enumerate(joke_loader):\n",
        "\n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
        "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "\n",
        "        #The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            #The next joke does not fit in so we process the sequence and leave the last joke\n",
        "            #as the start for next sequence\n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "\n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss, logits = outputs[:2]\n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "\n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0\n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 100:\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "\n",
        "    # Store the model after each epoch to compare the performance of them\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "CxCatiaOeevq",
        "execution": {
          "iopub.status.busy": "2024-04-07T22:57:06.418099Z",
          "iopub.execute_input": "2024-04-07T22:57:06.418858Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_EPOCH = 4\n",
        "\n",
        "models_folder = \"trained_models\"\n",
        "\n",
        "model_path = os.path.join(models_folder, f\"gpt2_medium_joker_{MODEL_EPOCH}.pt\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "jokes_output_file_path = f'generated_{MODEL_EPOCH}.jokes'\n",
        "\n",
        "model.eval()\n",
        "if os.path.exists(jokes_output_file_path):\n",
        "    os.remove(jokes_output_file_path)\n",
        "\n",
        "joke_num = 0\n",
        "with torch.no_grad():\n",
        "\n",
        "        for joke_idx in range(1000):\n",
        "\n",
        "            joke_finished = False\n",
        "\n",
        "            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n",
        "\n",
        "            for i in range(100):\n",
        "                outputs = model(cur_ids, labels=cur_ids)\n",
        "                loss, logits = outputs[:2]\n",
        "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
        "                if i < 3:\n",
        "                    n = 20\n",
        "                else:\n",
        "                    n = 3\n",
        "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
        "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
        "\n",
        "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "                    joke_finished = True\n",
        "                    break\n",
        "\n",
        "\n",
        "            if joke_finished:\n",
        "\n",
        "                joke_num = joke_num + 1\n",
        "\n",
        "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "                output_text = tokenizer.decode(output_list)\n",
        "\n",
        "                with open(jokes_output_file_path, 'a') as f:\n",
        "                    f.write(f\"{output_text} \\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ev1fBTbYewDM",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIYAvp0QkfLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xaxKMUBBGz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}