{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAITAME/DL_Lab-3/blob/main/lab3_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:25.726989Z",
          "iopub.execute_input": "2024-04-08T00:57:25.727642Z",
          "iopub.status.idle": "2024-04-08T00:57:25.733110Z",
          "shell.execute_reply.started": "2024-04-08T00:57:25.727614Z",
          "shell.execute_reply": "2024-04-08T00:57:25.732135Z"
        },
        "trusted": true,
        "id": "R0UPHJd4FqMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Liste des liens à scraper\n",
        "urls = [\n",
        "    \"https://mawdoo3.com/%D8%AA%D8%B9%D8%B1%D9%8A%D9%81_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
        "    \"https://mawdoo3.com/%D8%AE%D8%B5%D8%A7%D8%A6%D8%B5_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
        "    \"https://mawdoo3.com/%D9%85%D8%AC%D8%A7%D9%84%D8%A7%D8%AA_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
        "    \"https://innovationhub.social/articles/impact17_01\",\n",
        "    \"https://onpassive.com/blog/ar/why-the-growth-of-artificial-intelligence-in-the-art-industry-wont-eliminate-artists#:~:text=%D8%A7%D9%84%D8%A5%D8%AC%D8%A7%D8%A8%D8%A9%20%D8%B9%D9%84%D9%89%20%D9%85%D8%A7%20%D8%A5%D8%B0%D8%A7%20%D9%83%D8%A7%D9%86%20%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1%20%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A%20%D8%B3%D9%8A%D8%AD%D9%84,%D8%A7%D9%84%D8%A5%D8%A8%D8%AF%D8%A7%D8%B9%D9%8A%D8%A9%20%D8%A3%D8%B3%D9%87%D9%84%20%D9%88%D8%A3%D9%83%D8%AB%D8%B1%20%D9%81%D8%A7%D8%B9%D9%84%D9%8A%D8%A9%20%D9%85%D8%B9%20%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85%20%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1%20%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A.\",\n",
        "    \"https://mawdoo3.com/%D8%A8%D8%AD%D8%AB_%D8%B9%D9%86_%D9%85%D8%AE%D8%A7%D8%B7%D8%B1_%D8%A7%D9%84%D8%A5%D9%86%D8%AA%D8%B1%D9%86%D8%AA\",\n",
        "    \"https://onpassive.com/blog/ar/learn-about-responsible-ai\",\n",
        "    \"https://mawdoo3.com/%D8%AE%D8%B5%D8%A7%D8%A6%D8%B5_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1\",\n",
        "    \"https://mawdoo3.com/%D9%85%D9%86_%D9%87%D9%88_%D9%85%D8%AE%D8%AA%D8%B1%D8%B9_%D8%A7%D9%84%D9%83%D9%87%D8%B1%D8%A8%D8%A7%D8%A1\",\n",
        "    \"https://mawdoo3.com/%D8%A3%D9%87%D9%85%D9%8A%D8%A9_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A_%D9%81%D9%8A_%D9%85%D8%AC%D8%A7%D9%84_%D8%A7%D9%84%D8%AA%D8%B9%D9%84%D9%8A%D9%85\"\n",
        "\n",
        "\n",
        "]\n",
        "texts = []\n",
        "\n",
        "i = 0\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        content = soup.get_text()\n",
        "        content = content.replace('\\n\\n', '')\n",
        "        texts.append(content)\n",
        "    else:\n",
        "        print(f\"Échec de la requête à l'URL : {url}\")\n",
        "        print(f\"num : {i}\")\n",
        "\n",
        "    i = i+1\n",
        "\n",
        "df = pd.DataFrame(texts, columns=['Text'])\n",
        "\n",
        "df\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:26.532062Z",
          "iopub.execute_input": "2024-04-08T00:57:26.532766Z",
          "iopub.status.idle": "2024-04-08T00:57:31.073322Z",
          "shell.execute_reply.started": "2024-04-08T00:57:26.532737Z",
          "shell.execute_reply": "2024-04-08T00:57:31.072261Z"
        },
        "trusted": true,
        "id": "d-_yANB7FqML",
        "outputId": "f40f2936-d7d1-4dfe-e61d-0f7980013b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                Text\n0  تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...\n1  خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...\n2  مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...\n3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...\n4  \\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...\n5  بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...\n6  \\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...\n7  خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...\n8  من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...\n9  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>الذكاء الاصطناعي في خدمة التنمية المستدامة - م...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores =[9,7,8,7,6,0,4,2,0,5]\n",
        "df['Score'] = scores\n",
        "df\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:31.075256Z",
          "iopub.execute_input": "2024-04-08T00:57:31.075700Z",
          "iopub.status.idle": "2024-04-08T00:57:31.088968Z",
          "shell.execute_reply.started": "2024-04-08T00:57:31.075672Z",
          "shell.execute_reply": "2024-04-08T00:57:31.087977Z"
        },
        "trusted": true,
        "id": "M3iRzvsXFqMN",
        "outputId": "e888123b-5a51-4117-a606-8b9b9a4d8da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                Text  Score\n0  تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      9\n1  خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      7\n2  مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...      8\n3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...      7\n4  \\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...      6\n5  بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...      0\n6  \\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...      4\n7  خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...      2\n8  من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...      0\n9  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...      5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>الذكاء الاصطناعي في خدمة التنمية المستدامة - م...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df.copy()\n",
        "df2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:31.090104Z",
          "iopub.execute_input": "2024-04-08T00:57:31.090378Z",
          "iopub.status.idle": "2024-04-08T00:57:31.107236Z",
          "shell.execute_reply.started": "2024-04-08T00:57:31.090356Z",
          "shell.execute_reply": "2024-04-08T00:57:31.106114Z"
        },
        "trusted": true,
        "id": "mBa5O4ScFqMN",
        "outputId": "bddaf936-6fdf-4497-df7d-67a942361c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                Text  Score\n0  تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      9\n1  خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      7\n2  مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...      8\n3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...      7\n4  \\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...      6\n5  بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...      0\n6  \\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...      4\n7  خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...      2\n8  من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...      0\n9  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...      5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>الذكاء الاصطناعي في خدمة التنمية المستدامة - م...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "for c in string.punctuation:\n",
        "    print(c, end=\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:31.109525Z",
          "iopub.execute_input": "2024-04-08T00:57:31.109862Z",
          "iopub.status.idle": "2024-04-08T00:57:31.116471Z",
          "shell.execute_reply.started": "2024-04-08T00:57:31.109826Z",
          "shell.execute_reply": "2024-04-08T00:57:31.115479Z"
        },
        "trusted": true,
        "id": "-9USTSwrFqMN",
        "outputId": "cd5a159a-cb27-4934-ccb8-6a39031ee5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:31.540451Z",
          "iopub.execute_input": "2024-04-08T00:57:31.540794Z",
          "iopub.status.idle": "2024-04-08T00:57:33.552430Z",
          "shell.execute_reply.started": "2024-04-08T00:57:31.540766Z",
          "shell.execute_reply": "2024-04-08T00:57:33.551618Z"
        },
        "trusted": true,
        "id": "8u-FsQHfFqMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:33.553824Z",
          "iopub.execute_input": "2024-04-08T00:57:33.554245Z",
          "iopub.status.idle": "2024-04-08T00:57:33.651356Z",
          "shell.execute_reply.started": "2024-04-08T00:57:33.554220Z",
          "shell.execute_reply": "2024-04-08T00:57:33.650539Z"
        },
        "trusted": true,
        "id": "CvQx3mkrFqMO",
        "outputId": "f1738a92-1e88-4f14-c4ef-e82277aec9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:33.652741Z",
          "iopub.execute_input": "2024-04-08T00:57:33.653137Z",
          "iopub.status.idle": "2024-04-08T00:57:33.726962Z",
          "shell.execute_reply.started": "2024-04-08T00:57:33.653103Z",
          "shell.execute_reply": "2024-04-08T00:57:33.726156Z"
        },
        "trusted": true,
        "id": "oP8xxgKzFqMO",
        "outputId": "53b29c42-f1d6-4884-eff2-6d1272c42b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:33.728672Z",
          "iopub.execute_input": "2024-04-08T00:57:33.729094Z",
          "iopub.status.idle": "2024-04-08T00:57:34.993390Z",
          "shell.execute_reply.started": "2024-04-08T00:57:33.729065Z",
          "shell.execute_reply": "2024-04-08T00:57:34.992462Z"
        },
        "trusted": true,
        "id": "ytbS_RxBFqMO",
        "outputId": "5917f6bf-0870-45eb-dd72-6ea44b719ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.fileids()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:34.995453Z",
          "iopub.execute_input": "2024-04-08T00:57:34.995764Z",
          "iopub.status.idle": "2024-04-08T00:57:35.006670Z",
          "shell.execute_reply.started": "2024-04-08T00:57:34.995737Z",
          "shell.execute_reply": "2024-04-08T00:57:35.005755Z"
        },
        "trusted": true,
        "id": "ATdTd9stFqMP",
        "outputId": "d940e7e4-adad-485b-db7a-dfc5aa56ced2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['arabic',\n 'azerbaijani',\n 'basque',\n 'bengali',\n 'catalan',\n 'chinese',\n 'danish',\n 'dutch',\n 'english',\n 'finnish',\n 'french',\n 'german',\n 'greek',\n 'hebrew',\n 'hinglish',\n 'hungarian',\n 'indonesian',\n 'italian',\n 'kazakh',\n 'nepali',\n 'norwegian',\n 'portuguese',\n 'romanian',\n 'russian',\n 'slovene',\n 'spanish',\n 'swedish',\n 'tajik',\n 'turkish']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('arabic'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:35.007757Z",
          "iopub.execute_input": "2024-04-08T00:57:35.008062Z",
          "iopub.status.idle": "2024-04-08T00:57:35.019660Z",
          "shell.execute_reply.started": "2024-04-08T00:57:35.008038Z",
          "shell.execute_reply": "2024-04-08T00:57:35.018858Z"
        },
        "trusted": true,
        "id": "nf3Ij52PFqMP",
        "outputId": "e806c073-95d5-4dfc-cb63-b1c9935dc3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
        "    # Supprimer les caractères qui ne sont pas en arabe\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('arabic'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    cleaned_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "df2['clean_text'] = df2['Text'].apply(clean_text)\n",
        "df2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:35.470563Z",
          "iopub.execute_input": "2024-04-08T00:57:35.471320Z",
          "iopub.status.idle": "2024-04-08T00:57:37.669002Z",
          "shell.execute_reply.started": "2024-04-08T00:57:35.471290Z",
          "shell.execute_reply": "2024-04-08T00:57:37.668035Z"
        },
        "trusted": true,
        "id": "pVOBBje8FqMP",
        "outputId": "6c19e989-5538-4b07-f850-3a9e2337922a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                Text  Score  \\\n0  تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      9   \n1  خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...      7   \n2  مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...      8   \n3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...      7   \n4  \\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...      6   \n5  بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...      0   \n6  \\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...      4   \n7  خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...      2   \n8  من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...      0   \n9  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...      5   \n\n                                          clean_text  \n0  تعريف الذكاء الاصطناعي موضوع التصنيفات أجدد ال...  \n1  خصائص الذكاء الاصطناعي موضوع التصنيفات أجدد ال...  \n2  مجالات الذكاء الاصطناعي موضوع التصنيفات أجدد ا...  \n3  الذكاء الاصطناعي خدمة التنمية المستدامة مبادرة...  \n4  الذكاء الاصطناعي والفن تأثيره ومستقبل الفنانين...  \n5  بحث مخاطر الإنترنت موضوع التصنيفات أجدد المقال...  \n6  المسؤولية والأخلاق والتحيز المسؤولية والأخلاق ...  \n7  خصائص الذكاء موضوع التصنيفات أجدد المقالات الأ...  \n8  مخترع الكهرباء موضوع التصنيفات أجدد المقالات ا...  \n9  أهمية الذكاء الاصطناعي مجال التعليم موضوع التص...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Score</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>تعريف الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>9</td>\n      <td>تعريف الذكاء الاصطناعي موضوع التصنيفات أجدد ال...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>خصائص الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأج...</td>\n      <td>7</td>\n      <td>خصائص الذكاء الاصطناعي موضوع التصنيفات أجدد ال...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>مجالات الذكاء الاصطناعي - موضوع \\nالتصنيفات\\nأ...</td>\n      <td>8</td>\n      <td>مجالات الذكاء الاصطناعي موضوع التصنيفات أجدد ا...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>الذكاء الاصطناعي في خدمة التنمية المستدامة - م...</td>\n      <td>7</td>\n      <td>الذكاء الاصطناعي خدمة التنمية المستدامة مبادرة...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nالذكاء الاصطناعي والفن: تأثيره ومستقبل الفنا...</td>\n      <td>6</td>\n      <td>الذكاء الاصطناعي والفن تأثيره ومستقبل الفنانين...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>بحث عن مخاطر الإنترنت - موضوع \\nالتصنيفات\\nأجد...</td>\n      <td>0</td>\n      <td>بحث مخاطر الإنترنت موضوع التصنيفات أجدد المقال...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\nالمسؤولية والأخلاق والتحيز المسؤولية والأخلا...</td>\n      <td>4</td>\n      <td>المسؤولية والأخلاق والتحيز المسؤولية والأخلاق ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>خصائص الذكاء - موضوع \\nالتصنيفات\\nأجدد المقالا...</td>\n      <td>2</td>\n      <td>خصائص الذكاء موضوع التصنيفات أجدد المقالات الأ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>من هو مخترع الكهرباء - موضوع \\nالتصنيفات\\nأجدد...</td>\n      <td>0</td>\n      <td>مخترع الكهرباء موضوع التصنيفات أجدد المقالات ا...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...</td>\n      <td>5</td>\n      <td>أهمية الذكاء الاصطناعي مجال التعليم موضوع التص...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfVec = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "result  = tfidfVec.fit_transform(df2['clean_text'])\n",
        "result.toarray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T02:13:43.458395Z",
          "iopub.execute_input": "2024-04-08T02:13:43.458819Z",
          "iopub.status.idle": "2024-04-08T02:13:43.487762Z",
          "shell.execute_reply.started": "2024-04-08T02:13:43.458775Z",
          "shell.execute_reply": "2024-04-08T02:13:43.486795Z"
        },
        "trusted": true,
        "id": "3ap-yZMcFqMQ",
        "outputId": "63b49b58-6a86-4e67-ba74-023c1aca54c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 58,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.03938203,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.01472515, 0.        ,\n        0.        ],\n       [0.01425661, 0.01425661, 0.01425661, ..., 0.        , 0.        ,\n        0.01211942]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tfid_bag_of_words = pd.DataFrame(result.toarray(), columns=tfidfVec.get_feature_names_out())\n",
        "Tfid_bag_of_words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T00:57:39.551943Z",
          "iopub.execute_input": "2024-04-08T00:57:39.552301Z",
          "iopub.status.idle": "2024-04-08T00:57:39.585614Z",
          "shell.execute_reply.started": "2024-04-08T00:57:39.552273Z",
          "shell.execute_reply": "2024-04-08T00:57:39.584762Z"
        },
        "trusted": true,
        "id": "rZoSRq6FFqMQ",
        "outputId": "1cdd2f5a-8d73-41dd-86ef-1ac3e5ded421"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   آبالتعليم   آببرامج   آبتحميل   آبتسجيل      آبحل       آخر     آخرها  \\\n0   0.000000  0.000000  0.000000  0.000000  0.000000  0.006368  0.000000   \n1   0.000000  0.000000  0.000000  0.000000  0.000000  0.013009  0.000000   \n2   0.000000  0.000000  0.000000  0.000000  0.000000  0.014560  0.000000   \n3   0.000000  0.000000  0.000000  0.000000  0.000000  0.018916  0.000000   \n4   0.000000  0.000000  0.000000  0.000000  0.000000  0.021072  0.018998   \n5   0.000000  0.000000  0.000000  0.034627  0.000000  0.038407  0.000000   \n6   0.000000  0.000000  0.000000  0.000000  0.000000  0.008937  0.000000   \n7   0.000000  0.000000  0.000000  0.000000  0.000000  0.009820  0.000000   \n8   0.000000  0.000000  0.000000  0.000000  0.000000  0.005444  0.000000   \n9   0.014257  0.014257  0.014257  0.000000  0.014257  0.010542  0.000000   \n\n       آلاء      آلات      آلان  ...    ينشئها      ينظر     ينعكس      ينمو  \\\n0  0.000000  0.025622  0.014643  ...  0.000000  0.000000  0.000000  0.000000   \n1  0.000000  0.026168  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n2  0.000000  0.000000  0.033478  ...  0.000000  0.000000  0.000000  0.000000   \n3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n4  0.000000  0.000000  0.000000  ...  0.018998  0.000000  0.000000  0.000000   \n5  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n6  0.000000  0.000000  0.000000  ...  0.000000  0.012087  0.000000  0.000000   \n7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.026559   \n8  0.014725  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n9  0.000000  0.010603  0.000000  ...  0.000000  0.000000  0.014257  0.000000   \n\n       يهدف      يوفر    يوفرها      يولد      يوما     يوميا  \n0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n2  0.000000  0.000000  0.000000  0.000000  0.039382  0.000000  \n3  0.021747  0.019026  0.025582  0.000000  0.000000  0.000000  \n4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n5  0.000000  0.000000  0.000000  0.000000  0.000000  0.029436  \n6  0.020550  0.017979  0.000000  0.000000  0.000000  0.000000  \n7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n8  0.000000  0.000000  0.000000  0.014725  0.000000  0.000000  \n9  0.000000  0.042412  0.000000  0.000000  0.000000  0.012119  \n\n[10 rows x 3856 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>آبالتعليم</th>\n      <th>آببرامج</th>\n      <th>آبتحميل</th>\n      <th>آبتسجيل</th>\n      <th>آبحل</th>\n      <th>آخر</th>\n      <th>آخرها</th>\n      <th>آلاء</th>\n      <th>آلات</th>\n      <th>آلان</th>\n      <th>...</th>\n      <th>ينشئها</th>\n      <th>ينظر</th>\n      <th>ينعكس</th>\n      <th>ينمو</th>\n      <th>يهدف</th>\n      <th>يوفر</th>\n      <th>يوفرها</th>\n      <th>يولد</th>\n      <th>يوما</th>\n      <th>يوميا</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.006368</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.025622</td>\n      <td>0.014643</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.013009</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.026168</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014560</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.033478</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.039382</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.018916</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.021747</td>\n      <td>0.019026</td>\n      <td>0.025582</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.021072</td>\n      <td>0.018998</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.018998</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.034627</td>\n      <td>0.000000</td>\n      <td>0.038407</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.029436</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008937</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.012087</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.020550</td>\n      <td>0.017979</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.009820</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.026559</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.005444</td>\n      <td>0.000000</td>\n      <td>0.014725</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014725</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.014257</td>\n      <td>0.014257</td>\n      <td>0.014257</td>\n      <td>0.000000</td>\n      <td>0.014257</td>\n      <td>0.010542</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.010603</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.014257</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.042412</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.012119</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 3856 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df2[[\"Score\"]]\n",
        "Y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T02:59:50.200741Z",
          "iopub.execute_input": "2024-04-08T02:59:50.201631Z",
          "iopub.status.idle": "2024-04-08T02:59:50.210566Z",
          "shell.execute_reply.started": "2024-04-08T02:59:50.201597Z",
          "shell.execute_reply": "2024-04-08T02:59:50.209681Z"
        },
        "trusted": true,
        "id": "OeZ0QNNwFqMQ",
        "outputId": "29ac0aa9-214a-4f80-8087-aa3906ad450e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 68,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Score\n0      9\n1      7\n2      8\n3      7\n4      6\n5      0\n6      4\n7      2\n8      0\n9      5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# X_train, X_test, y_train, y_test     = train_test_split(Tfid_bag_of_words , Y,test_size=0.3, random_state = 0)\n",
        "X_train, X_test, y_train, y_test     = train_test_split(result , Y.to_numpy(),test_size=0.3, random_state = 0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T03:04:29.955721Z",
          "iopub.execute_input": "2024-04-08T03:04:29.956166Z",
          "iopub.status.idle": "2024-04-08T03:04:29.963970Z",
          "shell.execute_reply.started": "2024-04-08T03:04:29.956136Z",
          "shell.execute_reply": "2024-04-08T03:04:29.962698Z"
        },
        "trusted": true,
        "id": "jy6ovthkFqMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.toarray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T03:04:30.813011Z",
          "iopub.execute_input": "2024-04-08T03:04:30.813746Z",
          "iopub.status.idle": "2024-04-08T03:04:30.820780Z",
          "shell.execute_reply.started": "2024-04-08T03:04:30.813714Z",
          "shell.execute_reply": "2024-04-08T03:04:30.819674Z"
        },
        "trusted": true,
        "id": "7qWajAzuFqMR",
        "outputId": "1ccde4c7-dc41-4499-d049-6cdbdb523ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 79,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[0.01425661, 0.01425661, 0.01425661, ..., 0.        , 0.        ,\n        0.01211942],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.02943585]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T03:27:08.904671Z",
          "iopub.execute_input": "2024-04-08T03:27:08.905045Z",
          "iopub.status.idle": "2024-04-08T03:27:22.933932Z",
          "shell.execute_reply.started": "2024-04-08T03:27:08.905019Z",
          "shell.execute_reply": "2024-04-08T03:27:22.932711Z"
        },
        "trusted": true,
        "id": "GKMgYishFqMR",
        "outputId": "4911a00d-7a46-499d-9722-19ad802ee9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.0.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming df2 is your DataFrame containing 'clean_text' and 'score' columns\n",
        "\n",
        "# Extracting clean_text and scores from the DataFrame\n",
        "texts = df2['clean_text'].values\n",
        "scores = df2['Score'].values\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to ensure uniform length\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Converting scores to numpy array\n",
        "labels = np.array(scores)\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float)\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Define your RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.rnn(embedded)\n",
        "        output = self.fc(hidden[-1])\n",
        "        return output\n",
        "\n",
        "# Initialize model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 64\n",
        "model = RNNModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(test_loader, desc='Testing'):\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T03:36:22.594245Z",
          "iopub.execute_input": "2024-04-08T03:36:22.594609Z",
          "iopub.status.idle": "2024-04-08T03:36:24.723872Z",
          "shell.execute_reply.started": "2024-04-08T03:36:22.594584Z",
          "shell.execute_reply": "2024-04-08T03:36:24.722771Z"
        },
        "trusted": true,
        "id": "aycAxUAGFqMS",
        "outputId": "a9db7c38-d820-4671-fa38-842b4ccc4ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 35.2311\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 34.1228\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 33.0563\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 32.0293\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 31.0367\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 30.0733\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 29.1342\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 28.2148\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 27.3113\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Training Loss: 26.4208\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 1/1 [00:00<00:00, 33.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 17.8395\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------"
      ],
      "metadata": {
        "id": "vZHcoomdFqMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming df2 is your DataFrame containing 'clean_text' and 'score' columns\n",
        "\n",
        "# Extracting clean_text and scores from the DataFrame\n",
        "texts = df2['clean_text'].values\n",
        "scores = df2['Score'].values\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences to ensure uniform length\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Converting scores to numpy array\n",
        "labels = np.array(scores)\n",
        "\n",
        "# Splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float)\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Define RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Define Bidirectional RNN model\n",
        "class BidirectionalRNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(BidirectionalRNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Define GRU model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Initialize models\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 64\n",
        "\n",
        "rnn_model = RNNModel(vocab_size, embedding_dim, hidden_dim)\n",
        "bidirectional_rnn_model = BidirectionalRNNModel(vocab_size, embedding_dim, hidden_dim)\n",
        "gru_model = GRUModel(vocab_size, embedding_dim, hidden_dim)\n",
        "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_rnn = optim.Adam(rnn_model.parameters())\n",
        "optimizer_bidirectional_rnn = optim.Adam(bidirectional_rnn_model.parameters())\n",
        "optimizer_gru = optim.Adam(gru_model.parameters())\n",
        "optimizer_lstm = optim.Adam(lstm_model.parameters())\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training RNN Model\n",
        "    rnn_model.train()\n",
        "    running_loss_rnn = 0.0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'RNN Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer_rnn.zero_grad()\n",
        "        outputs = rnn_model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer_rnn.step()\n",
        "        running_loss_rnn += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss_rnn = running_loss_rnn / len(train_loader.dataset)\n",
        "    print(f'RNN Training Loss: {epoch_loss_rnn:.4f}')\n",
        "\n",
        "    # Training Bidirectional RNN Model\n",
        "    bidirectional_rnn_model.train()\n",
        "    running_loss_bidirectional_rnn = 0.0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'Bidirectional RNN Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer_bidirectional_rnn.zero_grad()\n",
        "        outputs = bidirectional_rnn_model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer_bidirectional_rnn.step()\n",
        "        running_loss_bidirectional_rnn += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss_bidirectional_rnn = running_loss_bidirectional_rnn / len(train_loader.dataset)\n",
        "    print(f'Bidirectional RNN Training Loss: {epoch_loss_bidirectional_rnn:.4f}')\n",
        "\n",
        "    # Training GRU Model\n",
        "    gru_model.train()\n",
        "    running_loss_gru = 0.0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'GRU Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer_gru.zero_grad()\n",
        "        outputs = gru_model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer_gru.step()\n",
        "        running_loss_gru += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss_gru = running_loss_gru / len(train_loader.dataset)\n",
        "    print(f'GRU Training Loss: {epoch_loss_gru:.4f}')\n",
        "\n",
        "    # Training LSTM Model\n",
        "    lstm_model.train()\n",
        "    running_loss_lstm = 0.0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f'LSTM Epoch {epoch + 1}/{epochs}'):\n",
        "        optimizer_lstm.zero_grad()\n",
        "        outputs = lstm_model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer_lstm.step()\n",
        "        running_loss_lstm += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss_lstm = running_loss_lstm / len(train_loader.dataset)\n",
        "    print(f'LSTM Training Loss: {epoch_loss_lstm:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define function for evaluation\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_loader, desc='Testing'):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "# Evaluate RNN Model\n",
        "print('RNN Model Evaluation:')\n",
        "evaluate(rnn_model)\n",
        "\n",
        "# Evaluate Bidirectional RNN Model\n",
        "print('Bidirectional RNN Model Evaluation:')\n",
        "evaluate(bidirectional_rnn_model)\n",
        "\n",
        "# Evaluate GRU Model\n",
        "print('GRU Model Evaluation:')\n",
        "evaluate(gru_model)\n",
        "\n",
        "# Evaluate LSTM Model\n",
        "print('LSTM Model Evaluation:')\n",
        "evaluate(lstm_model)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-08T03:40:28.844114Z",
          "iopub.execute_input": "2024-04-08T03:40:28.844516Z",
          "iopub.status.idle": "2024-04-08T03:40:44.539629Z",
          "shell.execute_reply.started": "2024-04-08T03:40:28.844478Z",
          "shell.execute_reply": "2024-04-08T03:40:44.538693Z"
        },
        "trusted": true,
        "id": "QMeIuLtEFqMU",
        "outputId": "bcbb8ff5-2caa-426c-c56e-7a3a17849a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "RNN Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 36.8062\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 34.5779\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 32.7651\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 33.6776\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 33.7361\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 31.3049\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 31.0024\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 32.7338\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 30.9118\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 28.3283\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 29.3156\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 31.8187\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 28.3541\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 25.6572\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 27.7077\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 30.9271\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 26.0636\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 23.2833\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 26.1799\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 30.0530\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 24.0244\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 21.1845\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 24.7329\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 29.1911\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 22.2124\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 19.3324\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 23.3666\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 28.3371\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 20.6008\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 17.6990\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 22.0801\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 27.4885\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 19.1650\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 16.2596\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 20.8724\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 26.6436\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "RNN Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "RNN Training Loss: 17.8835\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Bidirectional RNN Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Bidirectional RNN Training Loss: 14.9939\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "GRU Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "GRU Training Loss: 19.7421\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "LSTM Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "LSTM Training Loss: 25.8019\nRNN Model Evaluation:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 1/1 [00:00<00:00, 32.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 12.8454\nBidirectional RNN Model Evaluation:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 1/1 [00:00<00:00, 17.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 12.2650\nGRU Model Evaluation:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 13.4219\nLSTM Model Evaluation:\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 1/1 [00:00<00:00, 53.26it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 16.9600\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqanJ5yGFqMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}